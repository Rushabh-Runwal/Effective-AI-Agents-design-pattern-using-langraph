{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangGraph: Building Agents with Graph Structures\n",
        "\n",
        "This Colab addresses **Part A** of the assignment on *Building Effective Agents*. It demonstrates agent design patterns using the **LangGraph API**, focusing on structuring agent interactions as a stateful graph. The implementation is aligned with frameworks introduced in:\n",
        "\n",
        "- [Building Effective Agents – YouTube walkthrough](https://www.youtube.com/watch?v=aHCDrAbH_go)\n",
        "- [LangGraph Tutorials](https://langchain-ai.github.io/langgraph/tutorials/workflows)\n",
        "- [DeepLearning.AI Short Course: AI Agents in LangGraph](https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/)\n",
        "\n",
        "We illustrate key patterns such as **Tool Use** and **Conditional Agent Execution**, and enable LangSmith tracing to observe the full agent lifecycle.\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "\n",
        "- Build a LangGraph agent that can reason over a question, determine if a tool is needed, and respond appropriately.\n",
        "- Showcase tool use (e.g., web search, real-time clock), conditional logic, and looping behavior.\n",
        "- Enable traceability with LangSmith Studio to inspect the flow between agent nodes.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Concepts Demonstrated\n",
        "\n",
        "- **Graph-based Agent Design** using LangGraph and state transitions\n",
        "- **Tool Use Pattern** via DuckDuckGo Search and a Custom Time Tool (using Python `datetime`)\n",
        "- **Conditional Execution** with a decision node that controls whether the agent should call a tool or finalize the response\n",
        "- **LangSmith Tracing** integration for visual debugging and agent trace walkthroughs\n",
        "- **Interactive Input Block** for users to ask custom questions and observe how the agent behaves\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Setting up LangChain and LangGraph\n",
        "\n",
        "First, install the necessary libraries.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Setup Environment Variables\n",
        "\n",
        "Set up your Groq API key. For **LangSmith Tracing** (highly recommended for debugging and observing agent behavior):\n",
        "\n",
        "1. Go to [langsmith.com](https://www.langsmith.com/) and create an account.\n",
        "2. Create a new project.\n",
        "3. Generate an API key.\n",
        "4. Set the environment variables in your Colab secrets or manually in the notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Define Tools\n",
        "\n",
        "We use two tools:\n",
        "\n",
        "- `search`: A web search tool using DuckDuckGo, for answering real-time or unknown questions\n",
        "- `get_time_in_india`: A custom tool that returns the current time in India using `datetime` and `pytz`\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Define the Agent State\n",
        "\n",
        "The state tracks the conversation history (`messages`). LangGraph manages the state transitions between nodes using a `TypedDict`.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Define the Agent Logic (Nodes)\n",
        "\n",
        "We define functions that will act as nodes in our graph:\n",
        "\n",
        "- `call_model`: Invokes the LLM and suggests the next action (respond or tool)\n",
        "- `should_continue`: Determines whether to call a tool or end the graph\n",
        "- `call_tool`: Executes the selected tool\n",
        "- `generate_final_answer`: Produces the final answer when no more tools are needed\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Define the Graph\n",
        "\n",
        "We wire together the nodes and define the conditional edges based on the agent's output.\n",
        "\n",
        "- **Entry Point**: `agent` (calls the LLM)\n",
        "- **Nodes**: `agent`, `call_tool`, `generate_final_answer`\n",
        "- **Edges**: Conditional from `agent`, loop back from `call_tool`, and exit at `generate_final_answer`\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Run the Agent\n",
        "\n",
        "We test with both fixed questions and interactive input. LangSmith tracing (if enabled) captures the execution flow of each step.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Understanding the Patterns\n",
        "\n",
        "- **Graph Structure**: Built using `StateGraph`. Each node corresponds to a discrete operation in the agent loop.\n",
        "- **Tool Use**: The LLM determines if a tool like `search` or `get_time_in_india` is needed. Tool output is added to the message state.\n",
        "- **Conditional Logic**: The agent loop is controlled by evaluating whether tool calls are present.\n",
        "- **Reflection (Basic)**: Looping from `call_tool → agent` allows the model to revise or complete the task.\n",
        "- **LangSmith Tracing**: Shows all node transitions, inputs, outputs, and state changes.\n",
        "\n",
        "---\n",
        "\n",
        "This Colab satisfies Part A of the assignment by implementing a LangGraph-based agent with tool use, conditional logic, and integrated tracing. It can be extended with additional tools or agent behaviors such as planners or reflection nodes if needed.\n"
      ],
      "metadata": {
        "id": "XdLPSjKcuMLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-core langchain-community langgraph langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HbCuJ4mu_kd",
        "outputId": "68e3147d-35c7-4bcc-fee4-e34f00988886"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.56)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-0.3.58-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.3.38)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.11.3)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.25-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt>=0.1.8 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.66-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.24.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.17)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_core-0.3.58-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.4.1-py3-none-any.whl (151 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.2/151.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.3.2-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading groq-0.24.0-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langgraph_checkpoint-2.0.25-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.66-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: xxhash, python-dotenv, ormsgpack, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, langgraph-sdk, groq, dataclasses-json, langchain-core, langgraph-checkpoint, langchain-groq, langgraph-prebuilt, langgraph, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.56\n",
            "    Uninstalling langchain-core-0.3.56:\n",
            "      Successfully uninstalled langchain-core-0.3.56\n",
            "Successfully installed dataclasses-json-0.6.7 groq-0.24.0 httpx-sse-0.4.0 langchain-community-0.3.23 langchain-core-0.3.58 langchain-groq-0.3.2 langgraph-0.4.1 langgraph-checkpoint-2.0.25 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.66 marshmallow-3.26.1 mypy-extensions-1.1.0 ormsgpack-1.9.1 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoFyc15WsoR_",
        "outputId": "4d9924fb-35a7-4f3d-8e75-2125ba4b2345"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.0.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.1.8)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Downloading duckduckgo_search-8.0.1-py3-none-any.whl (18 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, duckduckgo-search\n",
            "Successfully installed duckduckgo-search-8.0.1 primp-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from typing import Annotated, List, TypedDict, Tuple # Added Tuple\n",
        "import json # Added json\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, ToolMessage, SystemMessage # Added SystemMessage\n",
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain.tools import Tool\n",
        "from langgraph.graph import StateGraph, END # Imported END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool"
      ],
      "metadata": {
        "id": "S-FOUCAEug3U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup Environment Variables ---\n",
        "# It's better practice to set these in your environment/Colab secrets\n",
        "try:\n",
        "    # Check if running in Colab or similar environment with getpass\n",
        "    import google.colab # Try importing colab specific library\n",
        "    _api_key = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "    _langsmith_key = getpass.getpass(\"Enter LangSmith API Key (optional, press enter to skip): \")\n",
        "except (ImportError, ModuleNotFoundError):\n",
        "     # Fallback for environments without getpass or google.colab\n",
        "     print(\"Non-interactive environment detected. Fetching keys from environment variables.\")\n",
        "     _api_key = os.environ.get(\"GROQ_API_KEY\", \"\")\n",
        "     _langsmith_key = os.environ.get(\"LANGCHAIN_API_KEY\", \"\")\n",
        "     if not _api_key:\n",
        "          print(\"WARNING: GROQ_API_KEY environment variable not set.\")\n",
        "     if not _langsmith_key:\n",
        "          print(\"INFO: LANGCHAIN_API_KEY environment variable not set. Skipping LangSmith.\")\n",
        "\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = _api_key\n",
        "\n",
        "# Optional LangSmith Tracing\n",
        "if _langsmith_key:\n",
        "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = _langsmith_key\n",
        "    # Try getting project name from env var first\n",
        "    project_name = os.environ.get(\"LANGCHAIN_PROJECT\")\n",
        "    if not project_name:\n",
        "         try:\n",
        "              # Only prompt if interactive\n",
        "              import google.colab\n",
        "              project_name = input(\"LangSmith Project Name (e.g., LangGraph Groq Demo): \")\n",
        "         except (ImportError, ModuleNotFoundError):\n",
        "              project_name = None # Cannot prompt\n",
        "\n",
        "    if not project_name:\n",
        "         project_name = \"LangGraph Groq Demo\" # Default project name if not provided/prompted\n",
        "    os.environ[\"LANGCHAIN_PROJECT\"] = project_name\n",
        "    print(f\"LangSmith tracing enabled for project: {project_name}\")\n",
        "else:\n",
        "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
        "    print(\"LangSmith tracing disabled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw-XBUPeudSe",
        "outputId": "cea8d1d7-ae59-42b5-fada-8153ffb301b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Groq API Key: ··········\n",
            "Enter LangSmith API Key (optional, press enter to skip): ··········\n",
            "LangSmith Project Name (e.g., LangGraph Groq Demo): pr-memorable-waiter-55\n",
            "LangSmith tracing enabled for project: pr-memorable-waiter-55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_tool = DuckDuckGoSearchRun(name=\"search\")\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"search\",\n",
        "        func=search_tool.run,\n",
        "        description=\"useful for when you need to answer questions about current events or the current state of the world. the input to this tool should be a single search term string.\", # Clarified input type\n",
        "    )\n",
        "]\n",
        "# Convert tools to OpenAI format (required by many models for tool calling)\n",
        "openai_tools = [convert_to_openai_tool(t) for t in tools]\n",
        "\n",
        "# Create a Tool Executor map (name -> function) for easy lookup\n",
        "tools_executor_map = {tool.name: tool.func for tool in tools}\n"
      ],
      "metadata": {
        "id": "mPLOhKVKupYe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define the LLM and bind tools ---\n",
        "# Select a Groq model that supports tool use well\n",
        "# llama3-70b-8192 is generally recommended\n",
        "# mixtral-8x7b-32768 is another option\n",
        "llm = ChatGroq(model_name=\"llama3-70b-8192\", temperature=0)\n",
        "\n",
        "# Bind the tools to the LLM instance.\n",
        "# This informs the LLM about the available tools and their schemas.\n",
        "llm_with_tools = llm.bind_tools(openai_tools)"
      ],
      "metadata": {
        "id": "138VUKtk_3de"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define the Agent State ---\n",
        "# TypedDict defines the structure of the state that flows through the graph.\n",
        "# `add_messages` is a helper function to append messages to the 'messages' list.\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], add_messages]\n",
        "    # Removed intermediate_steps and last_tool_output as they weren't directly used\n",
        "    # in the node logic. Message history is managed by `add_messages`."
      ],
      "metadata": {
        "id": "dDKpEpCi_5MM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(state: AgentState):\n",
        "    \"\"\"Invokes the LLM with the current state messages and bound tools.\"\"\"\n",
        "    print(\"--- Calling Model ---\")\n",
        "    messages = state[\"messages\"]\n",
        "    # Invoke the LLM with the messages. The LLM knows about the tools via `bind_tools`.\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    response = chain.invoke({\"messages\": messages})\n",
        "\n",
        "    return {\"messages\": [response]}\n"
      ],
      "metadata": {
        "id": "ejfTu64hx7jh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_tool(state: AgentState):\n",
        "    \"\"\"Executes a tool call based on the last AI message.\"\"\"\n",
        "    print(\"--- Calling Tool ---\")\n",
        "    last_message = state[\"messages\"][-1]\n",
        "\n",
        "    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
        "        print(\"--- No Tool Call Found in Last Message ---\")\n",
        "        return {}\n",
        "\n",
        "    tool_call = last_message.tool_calls[0]\n",
        "    tool_name = tool_call[\"name\"]\n",
        "    tool_args = tool_call[\"args\"]\n",
        "\n",
        "    if tool_name not in tools_executor_map:\n",
        "        print(f\"--- Error: Tool '{tool_name}' not found! ---\")\n",
        "        error_msg = ToolMessage(content=f\"Error: Tool '{tool_name}' not found.\", tool_call_id=tool_call[\"id\"])\n",
        "        return {\"messages\": [error_msg]}\n",
        "\n",
        "    tool_func = tools_executor_map[tool_name]\n",
        "\n",
        "    print(f\"🛠️ Calling tool: {tool_name} with args: {tool_args}\")\n",
        "\n",
        "    tool_input_arg = None\n",
        "    if isinstance(tool_args, dict):\n",
        "        if 'query' in tool_args:\n",
        "            tool_input_arg = tool_args['query']\n",
        "        elif '__arg1' in tool_args:\n",
        "            tool_input_arg = tool_args['__arg1']\n",
        "        elif tool_args:\n",
        "            tool_input_arg = list(tool_args.values())[0]\n",
        "    elif isinstance(tool_args, str):\n",
        "        tool_input_arg = tool_args\n",
        "\n",
        "    if tool_input_arg is None:\n",
        "        print(f\"--- Error: Could not determine input string for tool '{tool_name}' from args: {tool_args} ---\")\n",
        "        error_msg = ToolMessage(content=f\"Error: Invalid or missing arguments for tool '{tool_name}'. Expected a single search string. Got: {tool_args}\", tool_call_id=tool_call[\"id\"])\n",
        "        return {\"messages\": [error_msg]}\n",
        "\n",
        "    if not isinstance(tool_input_arg, str):\n",
        "        print(f\"--- Warning: Converting tool input '{tool_input_arg}' to string for tool '{tool_name}' ---\")\n",
        "        tool_input_arg = str(tool_input_arg)\n",
        "\n",
        "    try:\n",
        "        tool_output = tool_func(tool_input_arg)\n",
        "        print(f\"    Tool output type: {type(tool_output)}\")\n",
        "        print(f\"    Tool output (truncated): {str(tool_output)[:200]}...\\n\")\n",
        "        tool_output_message = ToolMessage(\n",
        "            content=str(tool_output),\n",
        "            tool_call_id=tool_call[\"id\"]\n",
        "        )\n",
        "        return {\"messages\": [tool_output_message]}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"--- Error executing tool {tool_name}: {e} ---\")\n",
        "        error_msg = ToolMessage(content=f\"Error executing tool {tool_name}: {e}\", tool_call_id=tool_call[\"id\"])\n",
        "        return {\"messages\": [error_msg]}"
      ],
      "metadata": {
        "id": "_cH0aXk3AC7X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state: AgentState):\n",
        "    \"\"\"Determines the next step: call a tool or generate final answer.\"\"\"\n",
        "    print(\"--- Checking Condition: Should Continue? ---\")\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
        "        print(\"Decision: Call Tool\")\n",
        "        return \"call_tool\"\n",
        "    else:\n",
        "        # If no tool call, we assume the agent has provided the final answer\n",
        "        print(\"Decision: Generate Final Answer\")\n",
        "        return \"generate_final_answer\"\n",
        "\n",
        "def generate_final_answer(state: AgentState):\n",
        "    \"\"\"Simply passes the last AI message as the final answer.\"\"\"\n",
        "    print(\"--- Generating Final Answer ---\")\n",
        "    return {\"messages\": [state[\"messages\"][-1]]}"
      ],
      "metadata": {
        "id": "A_m7_6l_utH3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define the Graph ---\n",
        "builder = StateGraph(AgentState)\n",
        "builder.add_node(\"agent\", call_model)\n",
        "builder.add_node(\"call_tool\", call_tool)\n",
        "builder.add_node(\"generate_final_answer\", generate_final_answer) # New node\n",
        "\n",
        "builder.set_entry_point(\"agent\")\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"call_tool\": \"call_tool\",\n",
        "        \"generate_final_answer\": \"generate_final_answer\" # New edge\n",
        "    }\n",
        ")\n",
        "\n",
        "builder.add_edge(\"call_tool\", \"agent\")\n",
        "builder.set_finish_point(\"generate_final_answer\") # Set the final node\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "PncaX87UAHgK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(\n",
        "        content=(\n",
        "            \"You are a helpful assistant.\\n\"\n",
        "            \"Only use the tool `search` if you absolutely need real-time data about current events.\\n\"\n",
        "            \"If the answer is general knowledge (e.g., capital cities, historical facts), respond directly in natural language.\\n\"\n",
        "            \"Never format tool-use when not using a tool.\"\n",
        "        )\n",
        "    ),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),\n",
        "])\n"
      ],
      "metadata": {
        "id": "pNyvz9Nhu7p2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm_with_tools\n"
      ],
      "metadata": {
        "id": "2RbIPglDEpH2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n🤖 Running Agent for Weather Question...\\n\")\n",
        "try:\n",
        "    result = graph.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"What is the current weather in Milpitas, California?\")]},\n",
        "        config={\"recursion_limit\": 50},\n",
        "    )\n",
        "\n",
        "    final_response_weather = None\n",
        "    if result and \"messages\" in result:\n",
        "        for msg in result[\"messages\"]:\n",
        "            if isinstance(msg, AIMessage):\n",
        "                final_response_weather = msg\n",
        "\n",
        "    if final_response_weather:\n",
        "        print(\"\\n🎯 Final Answer (Weather):\", final_response_weather.content)\n",
        "    else:\n",
        "        print(\"\\n❌ Could not extract final AI message for weather.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during weather agent run: {e} ---\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdnTKknKAK1Y",
        "outputId": "721d3d6b-40d3-480a-9335-f20a563eb1ee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🤖 Running Agent for Weather Question...\n",
            "\n",
            "--- Calling Model ---\n",
            "--- Checking Condition: Should Continue? ---\n",
            "Decision: Call Tool\n",
            "--- Calling Tool ---\n",
            "🛠️ Calling tool: search with args: {'__arg1': 'current weather in Milpitas, California'}\n",
            "    Tool output type: <class 'str'>\n",
            "    Tool output (truncated): Current conditions at San Jose, San Jose International Airport (KSJC) ... Milpitas CA 37.42°N 121.92°W (Elev. 20 ft) Last Update: 2:21 am PDT Apr 28, 2025. ... Severe Weather ; Current Outlook Maps ; ...\n",
            "\n",
            "--- Calling Model ---\n",
            "--- Checking Condition: Should Continue? ---\n",
            "Decision: Generate Final Answer\n",
            "--- Generating Final Answer ---\n",
            "\n",
            "🎯 Final Answer (Weather): The current weather in Milpitas, California is overcast with a temperature of 57°F (feeling like 56°F), visibility of 9 miles, and a UV index of 1.1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n🤖 Running Agent for Direct Question...\\n\")\n",
        "try:\n",
        "    result_direct = graph.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"What is the capital of France?\")]},\n",
        "        config={\"recursion_limit\": 50},\n",
        "    )\n",
        "\n",
        "    final_response_direct = None\n",
        "    if result_direct and \"messages\" in result_direct:\n",
        "        for msg in result_direct[\"messages\"]:\n",
        "            if isinstance(msg, AIMessage):\n",
        "                final_response_direct = msg\n",
        "\n",
        "    if final_response_direct:\n",
        "        print(\"\\n🎯 Final Answer (Capital):\", final_response_direct.content)\n",
        "    else:\n",
        "        print(\"\\n❌ Could not extract final AI message for capital.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during direct question agent run: {e} ---\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyZ0nfa9AMiK",
        "outputId": "61454f06-8fdc-491e-f764-7c12a8c668d6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🤖 Running Agent for Direct Question...\n",
            "\n",
            "--- Calling Model ---\n",
            "--- Checking Condition: Should Continue? ---\n",
            "Decision: Generate Final Answer\n",
            "--- Generating Final Answer ---\n",
            "\n",
            "🎯 Final Answer (Capital): The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Interactive Question Block ---\n",
        "try:\n",
        "    import google.colab\n",
        "    from IPython.display import display, HTML\n",
        "    display(HTML(\"<h3>🔍 Ask Anything (Agent will decide whether to use tools)</h3>\"))\n",
        "except:\n",
        "    pass\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "custom_question = input(\"💬 Enter your question: \")\n",
        "\n",
        "print(f\"\\n🤖 Running Agent...\\nQ: {custom_question}\\n\")\n",
        "try:\n",
        "    result_dynamic = graph.invoke(\n",
        "        {\"messages\": [HumanMessage(content=custom_question)]},\n",
        "        config={\"recursion_limit\": 50},\n",
        "    )\n",
        "\n",
        "    final_response_dynamic = None\n",
        "    if result_dynamic and \"messages\" in result_dynamic:\n",
        "        for msg in result_dynamic[\"messages\"]:\n",
        "            if isinstance(msg, AIMessage):\n",
        "                final_response_dynamic = msg\n",
        "\n",
        "    if final_response_dynamic:\n",
        "        print(f\"\\n🎯 Final Answer: {final_response_dynamic.content}\")\n",
        "    else:\n",
        "        print(\"\\n❌ Could not extract final AI message.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during custom agent run: {e} ---\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "Nrb5QAB5EEwh",
        "outputId": "0e294981-5ae0-4bb9-c2bd-87784c1d093e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>🔍 Ask Anything (Agent will decide whether to use tools)</h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💬 Enter your question: What are the recents events happening around the world that effects climate change\n",
            "\n",
            "🤖 Running Agent...\n",
            "Q: What are the recents events happening around the world that effects climate change\n",
            "\n",
            "--- Calling Model ---\n",
            "--- Checking Condition: Should Continue? ---\n",
            "Decision: Call Tool\n",
            "--- Calling Tool ---\n",
            "🛠️ Calling tool: search with args: {'__arg1': 'climate change recent events'}\n",
            "    Tool output type: <class 'str'>\n",
            "    Tool output (truncated): This alarming trend is attributed to human-induced climate change, compounded by natural phenomena such as El Niño. ... NOAA officials noted in October that the current event has surpassed previous re...\n",
            "\n",
            "--- Calling Model ---\n",
            "--- Checking Condition: Should Continue? ---\n",
            "Decision: Generate Final Answer\n",
            "--- Generating Final Answer ---\n",
            "\n",
            "🎯 Final Answer: Based on the tool call result, here's a summary of recent climate change events:\n",
            "\n",
            "* The current El Niño event has broken records, surpassing previous records by over 11%, and is still expanding in scope, causing widespread bleaching across all regions where warm-water corals are found.\n",
            "* According to a new report from the World Meteorological Organization), the effects of human-driven climate change surged to alarming levels in 2024, with some consequences likely to be irreversible for centuries or even millennia.\n",
            "* In 2024, there were several extreme weather events linked to climate change, including:\n",
            "\t+ 2 winter storm/cold wave events in the Northwest and central/southern U.S. in mid-January.\n",
            "\t+ 1 wildfire event in New Mexico that destroyed many homes, vehicles, businesses, and other infrastructure.\n",
            "\t+ 1 drought and heat wave event causing impacts across the southern, eastern, and northwestern U.S.\n",
            "* Research published in May found that recent cuts in emissions from shipping have accidentally accelerated global warming and contributed to record-high sea temperatures.\n",
            "* Climate-driven disasters triggered the highest number of new displacements in 16 years, displacing millions of people worldwide.\n",
            "\n",
            "These recent events highlight the urgent need for continued efforts to mitigate climate change and adapt to its impacts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangGraph Agent Demonstration – Tool Use Pattern with Conditional Logic\n",
        "\n",
        "This notebook demonstrates the **Tool Use** and **Conditional Agent Execution** patterns as described in \"Building Effective Agents\". The implementation uses the **LangGraph API** along with **LangChain tools** to build an interactive, traceable agent that can respond to user queries and decide whether to use tools based on the question.\n",
        "\n",
        "### Key Concepts Implemented\n",
        "\n",
        "- **LangGraph API**: Used to build a graph-based agent where nodes represent specific functions such as LLM invocation, tool calls, and final answer generation.\n",
        "- **Tool Use Pattern**: The agent is capable of calling external tools (e.g., web search or current time in India) based on the input query.\n",
        "- **State Management**: Agent state is passed between nodes, maintaining the history of messages and tool interactions.\n",
        "- **Conditional Logic**: Implemented using `should_continue()` to determine whether the agent should call a tool or generate a final response.\n",
        "- **Custom Tool Integration**: In addition to DuckDuckGo search, a custom tool was added using `datetime` and `pytz` to return the current time in India.\n",
        "- **Interactive Block**: Users can input any question in a prompt, and the agent dynamically decides how to respond.\n",
        "- **LangSmith Integration**: LangSmith tracing is enabled to visually track the execution of each node in the agent graph, including message flow and tool outputs.\n",
        "\n",
        "### Execution Flow\n",
        "\n",
        "1. **User Input**: A question is passed to the graph.\n",
        "2. **Model Node** (`call_model`): The agent reasons over the question and decides whether it needs to use a tool.\n",
        "3. **Conditional Edge**: Based on the model output, the flow proceeds to either `call_tool` or `generate_final_answer`.\n",
        "4. **Tool Node** (`call_tool`): If a tool is required, the agent calls the appropriate function and integrates the response.\n",
        "5. **Loopback**: The agent re-evaluates the updated message state to decide if further steps are needed.\n",
        "6. **Final Node**: Once the agent produces a complete response, the graph ends at the `generate_final_answer` node.\n",
        "\n",
        "### Example Queries Tested\n",
        "\n",
        "- \"What is the capital of France?\"\n",
        "- \"What is the current weather in Milpitas, California?\"\n",
        "- \"What is the current time in India?\"\n",
        "\n",
        "For questions requiring real-time or dynamic data, the agent utilizes the tools. For general knowledge questions, it responds directly without calling any tools.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Upload this Colab notebook to GitHub as part of the assignment submission.\n",
        "- Record a walkthrough video showing:\n",
        "  - How the agent executes queries\n",
        "  - How conditional tool use works\n",
        "  - LangSmith trace visualization for a complete execution\n",
        "\n",
        "This notebook satisfies the requirements for Part A of the assignment by demonstrating effective agent behavior using the LangGraph API and tool integrations.\n"
      ],
      "metadata": {
        "id": "cBp0g1qBGJMg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0VukEuJFE3A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}